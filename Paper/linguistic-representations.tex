The work of \citet{belinkov2020linguistic} is a significant step in presenting all approaches that examine the linguistic nature of the representations learned for MT in modern complex neural networks, specifically in LSTMs \citep{bahdanau2014neural}. The authors focus on three large categories of language properties, morphology, syntax and semantics, show that each type of information is better captured at a different layer of the architecture, and set the ground for further exploration of linguistic representations in MT with Transformer-based models \citep{vaswani2017attention}.