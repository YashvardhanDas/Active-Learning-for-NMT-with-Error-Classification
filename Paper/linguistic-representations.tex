The work of \citet{belinkov2020linguistic} is a significant step in collecting and interpreting all approaches that examine the linguistic nature of the representations learned by modern complex neural network Machine Translation models, specifically LSTMs \citep{bahdanau2014neural}. The authors focus on three large categories of language properties, morphology, syntax and semantics, show that each category of information is better captured at a different layer of the architecture, and set the ground for further exploration of linguistic representations of more state-of-the-art models, such as the Transformer \citep{vaswani2017attention}.