Active learning only selects the most informative instances from an unlabelled dataset
to expedite the learning process, making it effective in cases where unlabelled data is 
plentiful. Recent work has shown that representations of natural languages based on 
learned word embeddings can be useful for a wide range of NLP tasks, where 
transformer-based pre-trained models such as BERT (Bi-directional encoder representations 
from transformers), that are pre-trained with large-scale generic corpora and finetuned 
for specific tasks, have shown impressive results. However, there is little work devoted to 
leveraging word embeddings and transformer-based language models, though they are widely 
applied in text classification. \cite{lu2020investigating} addresses four primary research 
questions: 
\begin{enumerate}
    \item Are representations generated from pre-trained transformer language models more 
        effective than others with regards to active learning for text labelling?
    \item Is it required to use standard large models, or can lightweight models be used to 
        reduce the computational burden for active learning? Are there performance drops?
    \item Is it more effective to represent a document using aggregate of word embeddings 
        produced by the model or the CLS token? 
    \item Can the performance of the active learning system be improveed by using a 
        transformer-based model by fine-tuning? 
\end{enumerate}

\cite{lu2020investigating} uses an evaluation based on 8 datasets from different domains
to show that transformer-based language models consistently outperform more commonly used 
vector representations like word embeddings or bag-of-words. Comparing to BERT-like models, 
representations based on Roberta produce the best results. Additionally, averaged word 
representations are the most effective as compared to CLS tokens. An Adaptive Tuning 
Active Learning (ATAL) algorithm is proposed where labelled information is utilized for 
improving effectiveness of embeddings other than training classifiers. 