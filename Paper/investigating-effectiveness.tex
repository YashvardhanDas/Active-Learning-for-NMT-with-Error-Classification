Active learning only selects the most informative instances from an unlabelled dataset
to expedite the learning process, making it effective in cases where unlabelled data is 
plentiful. Recent work has shown that representations of natural languages based on 
learned word embeddings can be useful for a wide range of NLP tasks, where 
transformer-based pre-trained models such as BERT (Bi-directional encoder representations 
from transformers), that are pre-trained with large-scale generic corpora and finetuned 
for specific tasks, have shown impressive results. However, there is little work devoted to 
leveraging word embeddings and transformer-based language models, though they are widely 
applied in text classification. \cite{lu2020investigating} addresses four primary research 
questions: 
\begin{enumerate}
    \item Are representations generated from pre-trained transformer language models more 
        effective than others with regards to active learning for text labelling?
    \item 
\end{enumerate}