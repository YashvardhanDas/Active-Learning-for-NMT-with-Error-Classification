Active Learning (AL) has been widely studied in Natural Language Processing and integrated in modern systems as an effective approach in low resource scenaria for a variety of tasks. 
\citet{lu2020investigating} investigate AL in text classification, suggesting the effectiveness of representations generated from pre-trained Transformer models, compared to simple word embeddings.

In NMT, \citet{zhang2018active} propose two methods for sentence selection, considering either the semantic similarity between sentence embeddings or the decoder probabilities as tokens of translation quality, while \citet{peris2020active} explore AL in the interactive translation of large data streams. The latter work uses an NMT attention mechanism to query only for those sentences worthy of human annotation for translation. 

\citet{zeng2019empirical} perform an empirical evaluation of AL methods for Transformers, analyzing data- and model-driven sentences' sampling techniques. Their work notes the weak performance of Coverage Sampling, due to the complex nature of multi-layer multi-head architectures, an issue we are addressing with the current work.

A line of work has also explored the shortcomings of AL approaches in NMT. \citet{lowell2018practical} observe performance drop issues arising across different models and tasks and discuss the methods' generalization constraints. Similarly, \citet{koshorek2019limits} examine the potential and limitations of an oracle policy in selecting samples to maximize the performance of a NMT system in learning sentence semantic representations.


%Recent work in machine translation approaches has focused on integrating active learning
%with interaction-based neural machine translation. These modifications have been experimentally 
%employed with unbounded real-time streaming data \cite{peris2020active} with the objective of selecting 
%only those sentences which are very difficult for the translation system to accurately classify upon %
%during the initial stages. Results have shown promise in improving the overall quality of translation 
%and reducing human intervention in the process. A core component of this approach involves tweaking the 
%sampling strategies involved to maximize the probability of selecting only the most “difficult” sentences. 
%However, these results have a lack of representation and probable efficacy with respect to low-resource-based 
%multilingual paradigms. The effectiveness of slight alterations in the overall encoder-decoder transformer 
%network also needs to be experimented upon to possibly generate higher quality translations. s
